@article{mitton2025uncertainty,
  title={Uncertainty-Aware Knowledge Tracing Models},
  author={Mitton*, Joshua and Bhattacharyya*, Prarthana and Abboud, Ralph and Woodhead, Simon},
  journal={Pre-print, Under review},
  year={2025},
  selected={true},
  arxiv={2509.21514},
  preview={uncertainty_in_KT.png},
  abstract={New uncertainty-based approach that identifies struggling students where traditional metrics fail, enabling EdTech platforms to intervene at critical moments.}
}

@article{bhattacharyya2025helios,
  title={Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables},
  author={Bhattacharyya*, Prarthana and Mitton*, Joshua and Page*, Ryan and Morgan*, Owen and Powell*, Oliver and Menzies, Benjamin and Homewood, Gabriel and Jacobs, Kemi and Baesso, Paolo and Muhonen, Taru and others},
  journal={Pre-print, Under review},
  year={2025},
  selected={true},
  arxiv={2503.07825},
  pdf={https://openreview.net/forum?id=2sY0M7iZwR&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)},
  preview={helios2_system.jpg},
  abstract={Architected mobile-optimized models for smart glasses achieving 25x power reduction with 20% accuracy
gain (F1>80%). Profiled inference bottlenecks, implemented quantization and efficient architectures. Deployed
at CES-2025 in commercial development kit.},
  video={https://0e84f9dd10852326-tracking-platform-shared-public-assets.s3.eu-west-1.amazonaws.com/IMG_6222.mov}  
}

@article{bhattacharyya2024helios,
  title={Helios: An extremely low power event-based gesture recognition for always-on smart eyewear},
  author={Bhattacharyya*, Prarthana and Mitton*, Joshua and Page*, Ryan and Morgan, Owen and Menzies, Ben and Homewood, Gabriel and Jacobs, Kemi and Baesso, Paolo and Trickett, David and Mair, Chris and others},
  journal={Best Paper, Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  year={2024},
  selected={true},
  arxiv={2407.05206},
  pdf={https://link.springer.com/chapter/10.1007/978-3-031-91989-3_11},
  preview={HeliosComplete.jpg},
  abstract={Pioneered low-power low-latency ML models, representations and custom loss functions enabling micro-gesture detection for Meta-Raybans. Rapidly delivered a prototype within 6 months, publicly demonstrated at AWE-USA 2024 and featured by CNET. Won Best Paper award at ECCV-2024 Workshop.},
  video={https://0e84f9dd10852326-tracking-platform-shared-public-assets.s3.eu-west-1.amazonaws.com/IMG_1720.MOV}
}

@inproceedings{bhattacharyya2022ssl,
  title={SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous Driving},
  author={Bhattacharyya, Prarthana and Huang, Chengjie and Czarnecki, Krzysztof},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2022},
  selected={true},
  pdf={https://proceedings.mlr.press/v205/bhattacharyya23a/bhattacharyya23a.pdf},
  arxiv={2206.14116},
  abstract={Pioneered SSL methods for autonomous vehicle motion forecasting (80+ citations, 120+ GitHub stars, presented to Gatik AI). Achieves state-of-the-art prediction accuracy while maintaining lower computational complexity than transformer-based approaches.},
  preview={ssl-lanes.png},
  code={https://github.com/AutoVision-cloud/SSL-Lanes},

}

@inproceedings{bhattacharyya2021sa,
  title={Sa-Det3D: Self-Attention Based Context-Aware 3D Object Detection},
  author={Bhattacharyya, Prarthana and Huang, Chengjie and Czarnecki, Krzysztof},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops },
  pages={3022--3031},
  year={2021},
  selected={true},
  abstract={First method to combine self-attention with CNNs for 3D point cloud detection (90+ citations, 150+ GitHub stars).
  1.5 AP accuracy gain while slashing model size by 15-80% and compute by 30-50%. Proven across KITTI, nuScenes, and Waymo datasets.},
  preview={sadet3d.png},
  pdf={https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Bhattacharyya_SA-Det3D_Self-Attention_Based_Context-Aware_3D_Object_Detection_ICCVW_2021_paper.pdf},
  arxiv={2101.02672},
  code={https://github.com/AutoVision-cloud/SA-Det3D},
  video={https://www.youtube.com/watch?v=zC8TU0EG__4}
}

@inproceedings{baser2019fantrack,
  title={Fantrack: 3d multi-object tracking with feature association network},
  author={Baser, Erkan and Balasubramanian*, Venkateshwaran and Bhattacharyya*, Prarthana and Czarnecki, Krzysztof},
  booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)},
  pages={1426--1433},
  year={2019},
  organization={IEEE},
  selected={true},
  preview={fantrack.jpg},
  abstract={NeurIPS 2019 3D tracking competition winner (150+ citations).
  Replaces hand-crafted cost functions and complex optimization with a CNN that learns end-to-end. 
  Competitive multi-object tracking by automatically learning optimal assignments.},
  pdf={https://ieeexplore.ieee.org/abstract/document/8813779},
  arxiv={1905.02843},
  code={https://git.uwaterloo.ca/wise-lab/fantrack/-/tree/public_release?ref_type=heads},
}

@inproceedings{bhattacharyya2020deformable,
  title={Deformable PV-RCNN: Improving 3D Object Detection with Learned Deformations},
  author={Bhattacharyya, Prarthana and Czarnecki, Krzysztof},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  year={2020},
  arxiv={2008.08766},
  preview={deform_conv.png},
  code={https://github.com/AutoVision-cloud/Deformable-PV-RCNN},
  abstract={First method to integrate deformable convolutions in 3D object detection (40+ citations, 100+ GitHub stars).},
}

@inproceedings{bhattacharyya2022visual,
  title={Visual Representation Learning with Self-Supervised Attention for Low-Label High-data Regime},
  author={Bhattacharyya, Prarthana and Li, Chenge and Zhao, Xiaonan and Feh{\'e}rv{\'a}ri, Istv{\'a}n and Sun, Jason},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2022},
  preview={icassp2022.png},
  code={https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata},
  arxiv={2201.08951},
  pdf={https://assets.amazon.science/3d/a7/023eda3f4dd79cdabf474e11e8f6/visual-representation-learning-with-self-supervised-attention-for-low-label-high-data-regime.pdf},
  abstract={Developed self-supervised transformer framework reducing manual data annotation requirements. Achieved 4-11% improvement over SOTA on five benchmarks (ICASSP-2022).},
}

@inproceedings{bhattacharyya20173d,
  title={3D scene understanding at urban intersection using stereo vision and digital map},
  author={Bhattacharyya, Prarthana and Gu, Yanlei and Bao, Jiali and Liu, Xu and Kamijo, Shunsuke},
  booktitle={IEEE 85th Vehicular Technology Conference (VTC Spring)},
  pages={1--5},
  year={2017},
  organization={IEEE},
  arxiv={2112.05295},
  pdf={https://ieeexplore.ieee.org/abstract/document/8108283},
  preview={3d_scene_understanding.png},
  abstract={Proposed a novel method for 3D scene understanding at urban intersections by integrating stereo vision with digital map data. Tested on real-data collected from urban intersections in Tokyo.},
}

@phdthesis{bhattacharyya2023perception,
  title={Perception and Prediction in Multi-Agent Urban Traffic Scenarios for Autonomous Driving},
  author={Bhattacharyya, Prarthana},
  year={2023},
  school={University of Waterloo},
  journal={PhD Thesis, University of Waterloo},
  url={https://uwspace.uwaterloo.ca/items/eec7ea4d-67d9-4f97-b821-520798ce1ce4},
  html={https://uwspace.uwaterloo.ca/items/eec7ea4d-67d9-4f97-b821-520798ce1ce4},
  pdf={assets/pdf/Prarthana_Thesis_Defence_2023.pdf},
  video={https://www.youtube.com/watch?v=qFjiGt9I7GI},
  preview={thesis.jpg}
}

@INPROCEEDINGS{10588846,
  author={Bhattacharyya, Prarthana and Huang, Chengjie and Czarnecki, Krzysztof},
  booktitle={2024 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction}, 
  year={2024},
  pages={1450-1457},
  keywords={Measurement;Training;Pedestrians;Predictive models;Trajectory;Safety;Labeling;Index Terms: trajectory forecasting;automated driving;self-supervised learning;interaction modeling},
  pdf={https://ieeexplore.ieee.org/abstract/document/10588846},
  arxiv={2401.07729},
  preview={ssl_interactions.png},
  abstract={Self-supervised pretext tasks boost multi-agent interaction prediction by 8% in complex driving scenarios.}
  }

